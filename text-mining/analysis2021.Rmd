---
title: "Analysis"
output: html_notebook
---

# Step 0
Getting the data

In the following chunk of code, we used the data from the Living Repository - 2021 Excel file. We extracted the initiatives whose Status are 'Sent to evaluators' OR 'Information complete'. The main objective of this document is to find the initiatives that are similar (or different) to each other word_wise; i.e. the similarity degree of each pair of initiatives with regards to the words that used to explain their Description and/or thier Mission.
```{r setup, include=FALSE}
library(tidyverse)
library(tidytext)
library(stringr)
library(tm)
library(readxl)
library(wordcloud)
library(RColorBrewer)
library(topicmodels)
library(googlesheets4)
library(ldatuning)
library(ggplot2)
library(janitor)
library(readxl)


data = read_xlsx("./data/GPAI AIPR 2021 - Living Repository.xlsx", sheet = "Living Repository 2021")

data = data %>% 
  filter(Status %in% c('Sent to evaluators', 'Information complete'))
```



# Step 1
After reading the data from the Living Repository, we counted the number of words used in the "Description" and the "Mission" columns. The aim of the following chunk of code is counting the number of times each word has been used in each initiative. Note that some of the common words such as 'covid', '19', '2' or '50' have been excluded. 
```{r}
text_data = data %>% 
  mutate(initiative = row_number()) %>% 
  unite(text, Description:Mission, sep = " | ") %>% 
  select(initiative, text)

words = text_data %>% 
  unnest_tokens(word, text) 


custom_stopwords = c("covid","19","2","50")

words = words %>% 
  anti_join(stop_words) %>% 
  count(initiative, word, sort = TRUE) %>% 
  filter(!word %in% custom_stopwords ) 


words %>% 
  arrange(desc(n))
```

# Step 2
In this step, we considered the words that have been used more than 3 times. Note that these values are across the whole initiatives. For example, the word "data" has been used has been used more than 25 times across all the initiatives. 
```{r}
words %>%
  filter(n > 3) %>% 
  arrange(n) %>% 
  mutate(word = fct_reorder(word, desc(n))) %>% 
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity", fill = "#f68060", alpha = .6, width = .4) +
  coord_flip() +
  labs(y = NULL)
```

# Step 3
Now that we know the number of times each word has been used in each initiative and across all initiatives, we can find the number of clusters we can fit these initiatives into in terms of the similarities or differences regarding to these word. As it is shown in the following graphs, they have two lines each that cross each other at some points. This intersection point is the value that determines the number of clusters. The first graph has two intersections: 7 and 8. However, the second one has only one: 6. We would consider 8 cluster to make sure that all the differences and similarities have been taken into account. 
## LDA

## Generating Topics
```{r}
words_dtm = words %>% 
  cast_dtm(initiative, word, n)
```


```{r number_of_cluster}

# Compute the K value "scores" from K=4 to K=45
result = FindTopicsNumber(
    words_dtm,
    topics = seq(from = 2, to = 25, by = 1),
    metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
    method = "Gibbs",
    control = list(seed = 1948),
    mc.cores = 2L,
    verbose = TRUE
)

FindTopicsNumber_plot(result)
```

Now that we have the number of clusters that the words can be fitted in, we can see the members of each cluster. Note that some clusters overlap with each other. For instance, the word "data" can be fitted to almost all the clusters. 

```{r}
k = 6
topics = LDA(words_dtm,k = k)

word_topics = tidy(topics, matrix = "beta")

topic_terms = word_topics %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 10) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

topic_terms %>% 
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```


# Step 4 (Grouping initiatives (shown as document in the following code) by clusters (shown as topic in the following code))
So far we found the clusters for the words. But we have not grouped the initiatives yet. In this step we will determined that which initiative belongs to each cluster based on the similarities/differences of the words each initiative has. In the following code, "gamma" shows the degree at which each initiative belongs to each cluster. The lower the gamma value is, the less it is likely that that initiative (document) belongs to that cluster (topic).

```{r}
project_topics = tidy(topics, matrix = "gamma")
project_topics
```


# Step 5

In this step, we will create a Google Sheet that tells each word belongs to each cluster(s). We need this step in order to depict the clusters in a graphical format. 

```{r Word_Connections}

filtered_words = word_topics %>% 
  filter(beta > 0.01) 

unique_words = unique(filtered_words$term)
topic_values = rep(NA, length(unique_words))
for (i in 1:length(topic_values)) {
  word = unique_words[i]
  values = filtered_words %>% 
    filter(term == word)
  values = values$topic
  values = str_c("Topic ", values)
  topics = str_c(values, collapse = "|")
  topic_values[[i]] = topics
}
word_rows = tibble(
  Label = unique_words, 
  Type = "Word",
  Topic = topic_values
  )

topic_labels = 1:k
topic_labels = str_c("Topic ", topic_labels)

topic_rows = tibble(
  Label = topic_labels,
  Type = "Topic",
  Topic = NA,
)

word_output = bind_rows(word_rows, topic_rows)

write_sheet(word_output, "https://docs.google.com/spreadsheets/d/1pNwt1-LH5ksxTPJJGO5wMFdjr7O14Bct5MvWm9uhUC8/edit#gid=0")

```

# Step 6
In this step, we will create a Google Sheet that tells each initiative belongs to each cluster(s). We need this step in order to depict the clusters in a graphical format. 

```{r}
filtered_projects = project_topics %>% 
  filter(gamma > 0.01) %>% 
  mutate(Project = str_c("Project ", document)) %>% 
  select(Project, topic)

projects = str_c("Project ", 1:25)
topic_values = rep(NA, length(projects))
for (i in 1:length(projects)) {
  project = projects[i]
  values = filtered_projects %>% 
    filter(Project == project)
  values = values$topic
  values = str_c("Topic ", values)
  topics = str_c(values, collapse = "|")
  topic_values[[i]] = topics
}
project_rows = tibble(
  Label = projects, 
  Type = "Project",
  Topic = topic_values
  )

project_data = data %>% 
  mutate(Label = str_c("Project ", row_number())) %>% 
  select(Label, Description, Domain, Mission, Name) 
  

project_rows = left_join(project_rows, project_data, by = "Label")

project_rows = project_rows %>% 
  select(-Label) %>% 
  rename(Label = Name) %>% 
  select(Label, Type, Description, Topic, Domain, Mission)

project_output = bind_rows(project_rows, topic_rows) %>% 
  mutate(Domain = replace_na(Domain, "Topic"))
write_sheet(project_output, "https://docs.google.com/spreadsheets/d/1p4QQaYk465-qHQmk-Wsh28dlk-z6iAkw6IJC4Dx1XFQ/edit#gid=0", sheet = "project_output")

```


# Step 7

In this step, we will create a matrix that tells the degree of similarities/differences between each pair of initiatives. For example, initiative number 15 and 3 are 13% similar corresponding to the word similarities (in terms of Description and Mission definitions). 

```{r}
library(SnowballC)
library(stopwords)
library(text2vec)

tfidf_words = words %>% 
  select(-n) %>% 
  mutate(word = wordStem(word, language = "en")) %>% 
  count(initiative, word, sort = TRUE) %>% 
  bind_tf_idf(word, initiative, n)

sparse_matrix = tfidf_words %>% 
  cast_sparse(initiative, word, tf)


sparse_matrix[1:10, 1:4]


similarities = sim2(sparse_matrix, method = "cosine", norm = "l2")
#similarities[1:10, 1:4]
as.matrix(similarities)
```


```{r}
library(reshape2)
melted_matrix =  melt(as.matrix(similarities))
```

```{r}

similarity_pairs = melted_matrix %>% 
  mutate(self_pair = map2_lgl(Var1, Var2, function(var1, var2) var1 == var2)) %>% 
  filter(self_pair == FALSE) %>% 
  select(-self_pair) %>% 
  rename(initiative1 = Var1, initiative2 = Var2) %>% 
  arrange(desc(value))

similarity_pairs
```

# Step 8 (Adding domain)
Now that we have the clusters, we want to know how similar/different the initiatives are with regards to the Domain variable. We will do so by assigning weights to the two criteria we used so far named Domain and the similarity value between each pair (see above code >>> value column)

```{r}
catdoms = data %>% 
  select(Domain) %>% 
  mutate(initiative = row_number())

```

```{r}
similarity = similarity_pairs %>% 
  arrange(desc(value)) %>% 
  mutate(PrevP2 = lag(initiative1),PrevP1 = lag(initiative2),
         DuplicatedP1 = initiative1 == PrevP2,
         DuplicatedP2 = initiative2 == PrevP1,
         isDuplicated = DuplicatedP1 & DuplicatedP2,
         isDuplicated = replace_na(isDuplicated, FALSE)) %>% 
  filter(!isDuplicated) %>% 
  select(-PrevP2, -PrevP1, -DuplicatedP1, -DuplicatedP2, -isDuplicated)

```


```{r}
map_domain_similarity = function(initiative1, initiative2) {
  Project1Dom = filter(catdoms, initiative == initiative1)$Domain
  Project2Dom = filter(catdoms, initiative == initiative2)$Domain
  
  return(as.numeric(Project1Dom == Project2Dom))
}

similarity = similarity %>% 
  mutate(DomSimilarity = map2_dbl(initiative1, initiative2, map_domain_similarity))

```


```{r}
text_weight = 0.01
#cat_weight = 0.7
dom_weight = 0.29

similarity = similarity %>% 
  rowwise() %>% 
  mutate(Overall = value * text_weight + DomSimilarity * dom_weight) %>% 
  ungroup() %>% 
  arrange(desc(Overall))
similarity


```


# Step 9

Now that we know each initiative belongs to which cluster(s). So, we can depict it in a graphical way. To do so, we will use Kumu. This has been done automatically by writing the dataframes to google sheets. The kumu map is automatically linked to the google sheet, and will update when it changes. 



